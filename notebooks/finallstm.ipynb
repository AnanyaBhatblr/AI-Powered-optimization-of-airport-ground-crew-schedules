{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"../data/output_with_combined_patterns.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare the data\n",
    "categorical_cols = ['task_ID', 'Gate_number', 'Floor_No', 'shift_no']\n",
    "df[categorical_cols] = df[categorical_cols].astype('category')\n",
    "\n",
    "sequences = []\n",
    "target = []\n",
    "scalers = {}  # Store scalers for each group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in df.groupby(categorical_cols, observed=False):  # Pass observed=False\n",
    "    demand_values = group['crew_demand'].values.reshape(-1, 1)\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaled_demand = scaler.fit_transform(demand_values)\n",
    "    scalers[name] = scaler  # Store the scaler for this group\n",
    "    scaled_demand = scaled_demand.flatten()\n",
    "\n",
    "    seq_length = 10\n",
    "    for i in range(len(scaled_demand) - seq_length):\n",
    "        sequences.append((name, scaled_demand[i:i + seq_length]))  # Store the name of the group with the sequence\n",
    "        target.append(scaled_demand[i + seq_length])\n",
    "\n",
    "# Prepare input and output data\n",
    "X = np.array([seq[1] for seq in sequences])\n",
    "y = np.array(target)\n",
    "groups = [seq[0] for seq in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test, groups_train, groups_test = train_test_split(\n",
    "    X, y, groups, test_size=0.2, random_state=42, stratify=groups\n",
    ")\n",
    "\n",
    "# Reshape data for LSTM\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build the LSTM model\n",
    "model = keras.Sequential([\n",
    "    keras.Input(shape=(X_train.shape[1], X_train.shape[2])),  # Define input shape\n",
    "    keras.layers.LSTM(64, activation='tanh', return_sequences=True),\n",
    "    keras.layers.LSTM(32, activation='tanh'),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - loss: 0.1896 - mae: 0.3483 - val_loss: 0.0905 - val_mae: 0.2347\n",
      "Epoch 2/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 0.0844 - mae: 0.2268 - val_loss: 0.0811 - val_mae: 0.2233\n",
      "Epoch 3/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 0.0771 - mae: 0.2172 - val_loss: 0.0824 - val_mae: 0.2287\n",
      "Epoch 4/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.0752 - mae: 0.2160 - val_loss: 0.0753 - val_mae: 0.2161\n",
      "Epoch 5/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.0731 - mae: 0.2135 - val_loss: 0.0736 - val_mae: 0.2150\n",
      "Epoch 6/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 0.0720 - mae: 0.2115 - val_loss: 0.0775 - val_mae: 0.2182\n",
      "Epoch 7/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 0.0698 - mae: 0.2081 - val_loss: 0.0721 - val_mae: 0.2124\n",
      "Epoch 8/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.0692 - mae: 0.2072 - val_loss: 0.0724 - val_mae: 0.2120\n",
      "Epoch 9/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.0681 - mae: 0.2051 - val_loss: 0.0703 - val_mae: 0.2107\n",
      "Epoch 10/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.0672 - mae: 0.2038 - val_loss: 0.0715 - val_mae: 0.2140\n",
      "Epoch 11/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 0.0669 - mae: 0.2041 - val_loss: 0.0688 - val_mae: 0.2068\n",
      "Epoch 12/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 0.0665 - mae: 0.2035 - val_loss: 0.0706 - val_mae: 0.2096\n",
      "Epoch 13/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 0.0655 - mae: 0.2021 - val_loss: 0.0673 - val_mae: 0.2036\n",
      "Epoch 14/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 0.0658 - mae: 0.2024 - val_loss: 0.0668 - val_mae: 0.2046\n",
      "Epoch 15/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 0.0653 - mae: 0.2020 - val_loss: 0.0681 - val_mae: 0.2051\n",
      "Epoch 16/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 0.0644 - mae: 0.1998 - val_loss: 0.0659 - val_mae: 0.2015\n",
      "Epoch 17/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.0641 - mae: 0.1998 - val_loss: 0.0662 - val_mae: 0.2020\n",
      "Epoch 18/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 0.0638 - mae: 0.1991 - val_loss: 0.0658 - val_mae: 0.2044\n",
      "Epoch 19/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 0.0629 - mae: 0.1981 - val_loss: 0.0657 - val_mae: 0.2019\n",
      "Epoch 20/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 0.0629 - mae: 0.1980 - val_loss: 0.0651 - val_mae: 0.2016\n",
      "Epoch 21/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 0.0625 - mae: 0.1977 - val_loss: 0.0654 - val_mae: 0.2017\n",
      "Epoch 22/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 0.0630 - mae: 0.1978 - val_loss: 0.0651 - val_mae: 0.2007\n",
      "Epoch 23/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.0622 - mae: 0.1970 - val_loss: 0.0651 - val_mae: 0.2026\n",
      "Epoch 24/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 0.0615 - mae: 0.1953 - val_loss: 0.0650 - val_mae: 0.2009\n",
      "Epoch 25/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - loss: 0.0618 - mae: 0.1962 - val_loss: 0.0658 - val_mae: 0.2030\n",
      "Epoch 26/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 0.0615 - mae: 0.1957 - val_loss: 0.0647 - val_mae: 0.2013\n",
      "Epoch 27/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - loss: 0.0614 - mae: 0.1959 - val_loss: 0.0646 - val_mae: 0.2011\n",
      "Epoch 28/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 0.0609 - mae: 0.1942 - val_loss: 0.0647 - val_mae: 0.2010\n",
      "Epoch 29/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 0.0600 - mae: 0.1936 - val_loss: 0.0668 - val_mae: 0.2021\n",
      "Epoch 30/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.0605 - mae: 0.1939 - val_loss: 0.0645 - val_mae: 0.2001\n",
      "Epoch 31/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.0605 - mae: 0.1940 - val_loss: 0.0641 - val_mae: 0.1996\n",
      "Epoch 32/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 0.0595 - mae: 0.1928 - val_loss: 0.0642 - val_mae: 0.2016\n",
      "Epoch 33/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.0595 - mae: 0.1924 - val_loss: 0.0659 - val_mae: 0.2018\n",
      "Epoch 34/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.0586 - mae: 0.1915 - val_loss: 0.0642 - val_mae: 0.2008\n",
      "Epoch 35/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.0587 - mae: 0.1911 - val_loss: 0.0647 - val_mae: 0.2009\n",
      "Epoch 36/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.0588 - mae: 0.1914 - val_loss: 0.0645 - val_mae: 0.2007\n",
      "Epoch 37/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.0580 - mae: 0.1898 - val_loss: 0.0643 - val_mae: 0.2011\n",
      "Epoch 38/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.0577 - mae: 0.1897 - val_loss: 0.0646 - val_mae: 0.2015\n",
      "Epoch 39/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 0.0581 - mae: 0.1906 - val_loss: 0.0658 - val_mae: 0.2031\n",
      "Epoch 40/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 0.0575 - mae: 0.1888 - val_loss: 0.0649 - val_mae: 0.2012\n",
      "Epoch 41/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 0.0569 - mae: 0.1882 - val_loss: 0.0654 - val_mae: 0.2009\n",
      "Epoch 42/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 0.0568 - mae: 0.1884 - val_loss: 0.0649 - val_mae: 0.1999\n",
      "Epoch 43/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.0568 - mae: 0.1882 - val_loss: 0.0648 - val_mae: 0.2004\n",
      "Epoch 44/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.0567 - mae: 0.1878 - val_loss: 0.0657 - val_mae: 0.2028\n",
      "Epoch 45/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.0558 - mae: 0.1864 - val_loss: 0.0654 - val_mae: 0.2020\n",
      "Epoch 46/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.0560 - mae: 0.1871 - val_loss: 0.0651 - val_mae: 0.2025\n",
      "Epoch 47/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.0557 - mae: 0.1865 - val_loss: 0.0647 - val_mae: 0.2017\n",
      "Epoch 48/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.0554 - mae: 0.1857 - val_loss: 0.0657 - val_mae: 0.2023\n",
      "Epoch 49/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.0549 - mae: 0.1851 - val_loss: 0.0649 - val_mae: 0.2012\n",
      "Epoch 50/50\n",
      "\u001b[1m2145/2145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.0547 - mae: 0.1851 - val_loss: 0.0654 - val_mae: 0.2020\n",
      "Mean Absolute Error on Scaled Test Set: 0.19937601685523987\n",
      "\u001b[1m596/596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Mean Absolute Error on Scaled Test Set: {mae}\")\n",
    "predictions_scaled = model.predict(X_test)\n",
    "\n",
    "# Transform predictions back to original scale\n",
    "predictions = []\n",
    "y_test_original = []\n",
    "groups_used = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, group in enumerate(groups_test):\n",
    "    scaler = scalers[group]\n",
    "    prediction_scaled = predictions_scaled[i].reshape(-1, 1)\n",
    "    prediction = scaler.inverse_transform(prediction_scaled).flatten()\n",
    "    prediction_adjusted = np.where(prediction < 0, np.ceil(prediction), np.floor(prediction))  # Adjust prediction based on sign\n",
    "    actual = scaler.inverse_transform(y_test[i].reshape(-1, 1)).flatten()\n",
    "    predictions.append(prediction_adjusted[0])\n",
    "    y_test_original.append(actual[0])\n",
    "    groups_used.append(group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error on Test Set: 0.9843750715643068\n",
      "Mean Absolute Error on Test Set: 0.7640732385499187\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate evaluation metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test_original, predictions))\n",
    "print(f\"Root Mean Squared Error on Test Set: {rmse}\")\n",
    "mae = mean_absolute_error(y_test_original, predictions)\n",
    "print(f\"Mean Absolute Error on Test Set: {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save predictions to a CSV file\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Task_ID': [group[0] for group in groups_used],\n",
    "    'Gate_number': [group[1] for group in groups_used],\n",
    "    'Floor_No': [group[2] for group in groups_used],\n",
    "    'Shift_no': [group[3] for group in groups_used],\n",
    "    'Actual': y_test_original,\n",
    "    'Predicted': predictions\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to ../data/predictions_output.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions_df.to_csv(\"../data/predictions_output.csv\", index=False)\n",
    "print(f\"Predictions saved to ../data/predictions_output.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example of a single prediction\n",
    "last_group = groups_test[-1]\n",
    "last_scaler = scalers[last_group]\n",
    "last_sequence = X_test[-1]\n",
    "last_sequence = last_sequence.reshape((1, last_sequence.shape[0], last_sequence.shape[1]))\n",
    "\n",
    "predicted_value_scaled = model.predict(last_sequence)\n",
    "predicted_value = last_scaler.inverse_transform(predicted_value_scaled).flatten()\n",
    "predicted_value_adjusted = np.where(predicted_value < 0, np.ceil(predicted_value), np.floor(predicted_value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Single Prediction Example ---\n",
      "Group: ('T-007', 28, 2, 2), Predicted next value: -1.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"\\n--- Single Prediction Example ---\")\n",
    "print(f\"Group: {last_group}, Predicted next value: {predicted_value_adjusted[0]:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
